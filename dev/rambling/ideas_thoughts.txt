Ideas / thoughts 

8/15 
- theres lots of datasets out there. My guess if every researcher / ML engineer could quickly leverage all datasets, they could benefit from pretraining 
-----> currently, it requires lots of manual work to adapt the datasets to be able to do this, as well as lots of hassle managing models 

- also, is there some unified source of all the text in the world? Eg, sort of like the OpenAI API, but instead returning a response to your prompt, it jus
funtions sort of as a firehose of data? not sure about the costs that would be required to run an api like this? 

- before all the LLM madness began to occur, i dreamed of a GUI for building ML pipelines. I figured if each component followed a speciified schema,
other peoples modules could be leveraged to speed up the process of building models. Now it seems as if an ML agent could take the place of 
adapting different code blocks to work with each other, to train models faster. 


8/16 

- sometimes there are outliers in the dataset. It would be cool if the DL framework would keep track of samples that tend to lead to strange properties / changes in activations/losses 

- sometimes when doing "research" i kinda just want to automate the process of running an experiment with different parameters or datasets
and do this all in parralel. it would be cool just to say "do this experiment with theses xyz changes, and it something goes wrong, try to fix it -> 
and the agent would try its best to carry out the experiements, and maybe even bring the researcher in the loop to debug. 

- also when doing experirments, it would be cool if i could link my chatgpt conversations that helped me build these different experiemtns.
I like the idea of using chatGPT convos as a sort of "unsupervised" form of documentation. The question say a lot about what im doing, and 
likely will have strong relevance once ive forgot all about what i was working on. The biggest thing ive been noticing is that as chatgpt 
has allowed me to do way more coding, ive got all this code that i didnt really personally write myself, and there is way more now, so 
ive been forgetting a lot about previous scripts ive written... 




- lots of times, certain libraries will just not work. I ususally do the prompting to get it to use a different library. then later, while 
doing a similar task, it goes back to using the original problem causing lib. 2 solutions come to mind, intelligent prompt suggestion based off previous issues, or just being able to quickly search over ALL my past code. at this rate, using github would take up so much of my time (im moving so much faster now) --- that makes me think, perhabs we need an Agent that can automatically back things up on github. prob disable --force lol. 


- being able to change things like lr, activation functions, optimizer, # epochs mid run without having to completely start over and change the 
pretrained model to the most recent one would be awesome 






8/17 

im starting to think i might have sufficient ideas to make something useful and worth building. 
however, i want to focus on making the highest utility thing for the most amount of people 
eg, i want to grow as fast as possible while also being as useful as possible (i think they are the same thing usually) 
So decisions made around what actually gets built will be viewed through this lense first and foremost 



- picking a batch size is pretty annoying. I constantly have to kinda guess based on my wandb logs etc etc. It would be cool if the dataset object would calculate this for me, starting out at 1, and then calculating max. of course in choline fashion, restarting automatically in the event of a failure. 
When other DL frameworks crash, we get back up and run again 





8/18 
- trained a model without setting my seed. Now i have to retrain. I think there could sort of be a LLM check on the tr script before every run looking 
for things like this. I guess like a model specilized for ML workflows. But again, Im not looking to get into a model iq measuring contest with Google or OpenAI etc etc 

---> also circling back to my ideas around a new model strucuture, saving the training script along with the model saves the seed that the model was trained on. As an alternative, could run an LLM over the tr script and parse out critical save info. Mistakes are possible tho.... 




so ive trained a few models in my day. Im no yann lecunn, but ive trained a few models. I'm not the kind of person to get tied up in details, but i can still handle them and cosider my organizational skills decent. The issue im having it's hard to know what information (no programatic) information to log about my experiment. It would be cool if every time i clicked run, a bot would ask me what ive changed, and ask about specific changes it sees in the code. 

The big thing im seeing is that many of these "gotchas" when it comes to training a model is not something that can be solved with a simple wrapper or framework. There needs to be an intelligent overwatch over the codebase and "runbase" -- otherwise theres just gonna be a bunch of failed runs. ML engineering is about accounting and organization. Training pipelines across different versions scale sort of like the attention mechanism -- quadratically 

v1*(v2 and comarisons to v1)*(GPT speed increasing need for organization)*(utilizing and mangaging random cloud systems and managing different systems) 
= difficult^4 


another challenge is working with super large models on a local system. Well actually is it?? can i put llama on my cpu??? 

8/19 
- i generally feel binaries are a unterutilized component on working with Python. If docker fails, this is a good second choice.
---- when a run is gonna take 36 hours, is it really that big of a deal if we just compile ur code before deploying it? 
------  the key thing here is u can do all ur debugging locally, and click deploy without doing any debugging server side. 



strange thought, but i feel the hierarchical structure of file systems are a bit annoying
something worth investigating is sort of a spatial UI layout of files, and they would be clustered by function
also could add the ability to sort files by creation date etc, regardless of the folder they are in. 



one of the big concerns around p2p systems is GPU security. As far as I know, theres no way to make this 100% secure 
but the question is, could we make it 99.99% secure? 







8/20 
- keep track of files used for val and tr can get confusing. It would be cool if it would pick a seed for you by default? This seems obvious I feel 


8/20 

-- going back to the weird 3 dimensional GPT conversion UI, it would be interesting to be able to take a git repo, or even 
the files on your computer, and visualize them in a 3 dimensional space, along with conversations you've had. It would be clustered 
by creating some embedding matrix, and possibly multiple embeddings combined. Or perhaps just retroactively feeding the model all the code you've written
and matching it with GPT convos you have had. I would use that! Context is all u need? 



8/21 
for a lot of my projects, i use models not only in my final product, but also to process other data for labeling
the process of running inference with the model is often unoptimized, and also sometimes requires effort to simply do the inference.
Anymore, i just give all my code to chatgpt, and it usually is able to spit something good out. 
so i guess this problem has mainly been solved. so preface the following with that... 
The idea is to have a wrapper that wraps around the regular nn.module, however, it would automatically convert to onnx,tflite, or whatever format offers the best peformance. 
another idea is that sometimes simply converting to onnx can offer 2x speedups (for fwd pass) 
in situations when using a frozen backbone model, i wonder if perhaps there are many scenarios where the conversion could be handled under the hood, and get a 2x speedup for the user 
just more ideas i want to get on paper, and this is probably low priority 






8/22 
- being able to schedule a run on my own local system would be nice too. im not sure what the best way to do this would be.
 My guess probably using git to actually code what will be tested, and then you would specify what commits you want to run 






8/27 

have been doing some ai focused research lately. lots of work on smaller datasets like cifar and MNIST 
ive noticed keeping track of experiments (and things ive done qualitatively) has been somewhat difficult 
id like something with a UI that shows me a timeline of what ive done. and summarizes what ive done 
at varying levels of granularity 

a github but better haha 

github is great and has been optimized for the fortune 500 

theres only so much you can feature engineer that sells to every F500 

is there an opening for the LLM-backed ai researcher 


8/28 

-- haha just saw this https://www.canva.dev/blog/engineering/we-put-half-a-million-files-in-one-git-repository-heres-what-we-learned/




