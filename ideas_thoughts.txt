Ideas / thoughts 

8/15 
- theres lots of datasets out there. My guess if every researcher / ML engineer could quickly leverage all datasets, they could benefit from pretraining 
-----> currently, it requires lots of manual work to adapt the datasets to be able to do this, as well as lots of hassle managing models 

- also, is there some unified source of all the text in the world? Eg, sort of like the OpenAI API, but instead returning a response to your prompt, it jus
funtions sort of as a firehose of data? not sure about the costs that would be required to run an api like this? 

- before all the LLM madness began to occur, i dreamed of a GUI for building ML pipelines. I figured if each component followed a speciified schema,
other peoples modules could be leveraged to speed up the process of building models. Now it seems as if an ML agent could take the place of 
adapting different code blocks to work with each other, to train models faster. 


8/16 

- sometimes there are outliers in the dataset. It would be cool if the DL framework would keep track of samples that tend to lead to strange properties / changes in activations/losses 

- sometimes when doing "research" i kinda just want to automate the process of running an experiment with different parameters or datasets
and do this all in parralel. it would be cool just to say "do this experiment with theses xyz changes, and it something goes wrong, try to fix it -> 
and the agent would try its best to carry out the experiements, and maybe even bring the researcher in the loop to debug. 

- also when doing experirments, it would be cool if i could link my chatgpt conversations that helped me build these different experiemtns.
I like the idea of using chatGPT convos as a sort of "unsupervised" form of documentation. The question say a lot about what im doing, and 
likely will have strong relevance once ive forgot all about what i was working on. The biggest thing ive been noticing is that as chatgpt 
has allowed me to do way more coding, ive got all this code that i didnt really personally write myself, and there is way more now, so 
ive been forgetting a lot about previous scripts ive written... 




- lots of times, certain libraries will just not work. I ususally do the prompting to get it to use a different library. then later, while 
doing a similar task, it goes back to using the original problem causing lib. 2 solutions come to mind, intelligent prompt suggestion based off previous issues, or just being able to quickly search over ALL my past code. at this rate, using github would take up so much of my time (im moving so much faster now) --- that makes me think, perhabs we need an Agent that can automatically back things up on github. prob disable --force lol. 


- being able to change things like lr, activation functions, optimizer, # epochs mid run without having to completely start over and change the 
pretrained model to the most recent one would be awesome 






8/17 

im starting to think i might have sufficient ideas to make something useful and worth building. 
however, i want to focus on making the highest utility thing for the most amount of people 
eg, i want to grow as fast as possible while also being as useful as possible (i think they are the same thing usually) 
So decisions made around what actually gets built will be viewed through this lense first and foremost 



- picking a batch size is pretty annoying. I constantly have to kinda guess based on my wandb logs etc etc. It would be cool if the dataset object would calculate this for me, starting out at 1, and then calculating max. of course in choline fashion, restarting automatically in the event of a failure. 
When other DL frameworks crash, we get back up and run again 





8/18 
- trained a model without setting my seed. Now i have to retrain. I think there could sort of be a LLM check on the tr script before every run looking 
for things like this. I guess like a model specilized for ML workflows. But again, Im not looking to get into a model iq measuring contest with Google or OpenAI etc etc 

---> also circling back to my ideas around a new model strucuture, saving the training script along with the model saves the seed that the model was trained on. As an alternative, could run an LLM over the tr script and parse out critical save info. Mistakes are possible tho.... 




so ive trained a few models in my day. Im no yann lecunn, but ive trained a few models. I'm not the kind of person to get tied up in details, but i can still handle them and cosider my organizational skills decent. The issue im having it's hard to know what information (no programatic) information to log about my experiment. It would be cool if every time i clicked run, a bot would ask me what ive changed, and ask about specific changes it sees in the code. 

The big thing im seeing is that many of these "gotchas" when it comes to training a model is not something that can be solved with a simple wrapper or framework. There needs to be an intelligent overwatch over the codebase and "runbase" -- otherwise theres just gonna be a bunch of failed runs. ML engineering is about accounting and organization. Training pipelines across different versions scale sort of like the attention mechanism -- quadratically 

v1*(v2 and comarisons to v1)*(GPT speed increasing need for organization)*(utilizing and mangaging random cloud systems and managing different systems) 
= difficult^4 


another challenge is working with super large models on a local system. Well actually is it?? can i put llama on my cpu??? 




