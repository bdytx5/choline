Ideas / thoughts 

8/15 
- theres lots of datasets out there. My guess if every researcher / ML engineer could quickly leverage all datasets, they could benefit from pretraining 
-----> currently, it requires lots of manual work to adapt the datasets to be able to do this, as well as lots of hassle managing models 

- also, is there some unified source of all the text in the world? Eg, sort of like the OpenAI API, but instead returning a response to your prompt, it jus
funtions sort of as a firehose of data? not sure about the costs that would be required to run an api like this? 

- before all the LLM madness began to occur, i dreamed of a GUI for building ML pipelines. I figured if each component followed a speciified schema,
other peoples modules could be leveraged to speed up the process of building models. Now it seems as if an ML agent could take the place of 
adapting different code blocks to work with each other, to train models faster. 


8/16 

- sometimes there are outliers in the dataset. It would be cool if the DL framework would keep track of samples that tend to lead to strange properties / changes in activations/losses 

- sometimes when doing "research" i kinda just want to automate the process of running an experiment with different parameters or datasets
and do this all in parralel. it would be cool just to say "do this experiment with theses xyz changes, and it something goes wrong, try to fix it -> 
and the agent would try its best to carry out the experiements, and maybe even bring the researcher in the loop to debug. 
